{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c0a24-97df-4cf8-9435-bd9da9d1bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd53b7-5790-49a3-846b-68274acdfa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_csv(\"Hindi_English_Truncated_Corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33eb64-8033-4bc8-af95-78951f6fb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600dacb-5912-445e-902f-2c98ce3703a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[lines['source']=='ted']\n",
    "lines = lines[~pd.isnull(lines['english_sentence'])]\n",
    "lines.drop_duplicates(inplace=True)\n",
    "\n",
    "lines = lines.sample(n=25000, random_state=42)\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e2747a-1414-41f2-99fc-60eb408c98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.lower())\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfb5a2-520f-4b76-971f-1a998f0634e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\"'\", '',x))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23820d67-a2b5-4466-a84f-9b009f3a6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982314a-4391-4fe9-b4d7-4241c1fce460",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\"[1234567890]\", \"\", x))\n",
    "\n",
    "#remove extra spaces\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.strip())\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.strip())\n",
    "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\" +\",\" \", x))\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: 'START_' + x + '_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841a3ce-fcb9-428c-9d9a-00ecd054dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eng_words = set()\n",
    "for eng in lines['english_sentence']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_hindi_words=set()\n",
    "for hin in lines['hindi_sentence']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)\n",
    "\n",
    "lines['length_eng_sentence']= lines['english_sentence'].apply(lambda x: len(x.split(\" \")))\n",
    "lines['length_hin_sentence']=lines['hindi_sentence'].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3330ac-2f78-4d5e-a284-ff1047a091a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[lines['length_eng_sentence']<=20]\n",
    "lines = lines[lines['length_hin_sentence']<=20]\n",
    "max_length_src=max(lines['length_hin_sentence'])\n",
    "max_length_tar=max(lines['length_eng_sentence'])\n",
    "\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "num_encoder_tokens = num_decoder_tokens\n",
    "\n",
    "num_decoder_tokens += 1\n",
    "input_token_index = dict([(word, i +1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i +1) for i, word in enumerate(target_words)])\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "lines = shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5b32f-e30b-4d8f-889d-511d984c1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = lines['english_sentence'], lines['hindi_sentence']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "x_train.to_pickle('x_train.pkl')\n",
    "x_test.to_pickle('x_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e85cfc-bba5-46e5-b4bd-5ce73fb47c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x=x_train, y=y_train, batch_size = 128):\n",
    "    '''Generate a batch of data'''\n",
    "    while True:\n",
    "        for j in range(0, len(x), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src), dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar), dtype = 'float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(x[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t , word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word]\n",
    "\n",
    "                for t , word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word]\n",
    "                        if t>0:\n",
    "                            decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "\n",
    "latent_dim = 300\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state = True)\n",
    "encoder_output, state_h, state_c,  = encoder_lstm(enc_emb)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = Input(shape= (None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "train_samples = len(x_train)\n",
    "val_sample = len(x_test)\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58b3b6-fa33-4f56-a325-484d42311d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator = generate_batch(x_train, y_train, batch_size = batch_size),\n",
    "                   steps_per_epoch = train_samples//batch_size,\n",
    "                   epochs = epochs,\n",
    "                   validation_data = generate_batch(x_test, y_test, batch_size = batch_size),\n",
    "                   validation_steps = val_sample//batch_size)\n",
    "\n",
    "model.save_weights(nmt_weights.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e93b3-bd3c-4a42-ab8f-4c3829793f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "#decoder setup\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "#predict the next word\n",
    "decoder_outputs_2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state= decoder_states_inputs)\n",
    "decoder_state2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_state2)\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    state_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_moel.predict([target_seq] + state_value)\n",
    "        sampled_token_index = ap.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ''+ sampled_char\n",
    "        \n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) >50):\n",
    "            stop_condition = True\n",
    "            \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        state_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence\n",
    "\n",
    "train_gen = generate_batch(x_train, y_train, batch_size=1)\n",
    "k =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4aaca-ffc0-4c77-801d-ea04c14b5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k +=1\n",
    "(input_seq, actual_output), = next(train_gen)\n",
    "decoded_sentence = decoded_sequence(input_seq)\n",
    "print('Input English sentence:', x_train[k:k+1]. values[0])\n",
    "print('Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Prediction Translation:', decoded_sentence[:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
